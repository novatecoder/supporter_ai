import traceback
import redis.asyncio as redis # Redis ìƒíƒœ ì²´í¬ë¥¼ ìœ„í•´ ì¶”ê°€
from contextlib import asynccontextmanager
from typing import Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_core.messages import HumanMessage
from loguru import logger
from langgraph.checkpoint.redis.aio import AsyncRedisSaver 

from supporter_ai.graph.workflow import create_supporter_workflow
from supporter_ai.common.config import settings

# ì „ì—­ ìƒíƒœ ê´€ë¦¬
app_state: Dict[str, Any] = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ Redis ì—°ê²° ë° ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
    try:
        logger.info("ğŸš€ Supporter AI ì´ˆê¸°í™” (Redis Stack ì—°ê²° ì¤‘...)")
        redis_url = f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}"
        
        # 1. Redis ìƒíƒœ ì²´í¬ìš© í´ë¼ì´ì–¸íŠ¸ (Health checkìš©)
        app_state["redis_client"] = redis.from_url(redis_url, decode_responses=True)
        
        # 2. Async ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ LangGraph ì²´í¬í¬ì¸í„° ê´€ë¦¬
        async with AsyncRedisSaver.from_conn_string(redis_url) as saver:
            app_state["graph"] = await create_supporter_workflow(saver)
            logger.info("âœ… Redis Stack ì—°ê²° ë° ì›Œí¬í”Œë¡œìš° ë¡œë“œ ì™„ë£Œ.")
            yield 
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {traceback.format_exc()}")
        raise e
    finally:
        # ì¢…ë£Œ ì‹œ ì—°ê²° ì •ë¦¬
        if "redis_client" in app_state:
            await app_state["redis_client"].aclose()
        app_state.clear()

app = FastAPI(title="Supporter AI", lifespan=lifespan)

# --- [SECTION: ë°ì´í„° ëª¨ë¸] ---
class ChatRequest(BaseModel):
    user_id: str = "kwh_01"
    session_id: str = "sess_01"
    message: str = "ì•ˆë…•" # ê¸°ë³¸ ì˜ˆì‹œê°’

# --- [SECTION: API ì—”ë“œí¬ì¸íŠ¸] ---

@app.get("/health")
async def health_check():
    """
    ì„œë²„ ìƒíƒœ ë° ì£¼ìš” ì»´í¬ë„ŒíŠ¸(Redis, AI Engine) ì—°ê²° í™•ì¸
    """
    graph_ready = "graph" in app_state
    redis_ready = False
    
    try:
        # Redis PING í…ŒìŠ¤íŠ¸
        if "redis_client" in app_state:
            redis_ready = await app_state["redis_client"].ping()
    except Exception:
        redis_ready = False

    return {
        "status": "healthy" if graph_ready and redis_ready else "unhealthy",
        "project": "Supporter AI",
        "engine_ready": graph_ready,
        "redis_connected": redis_ready,
        "model": settings.LLM_MODEL_NAME #
    }

@app.post("/api/v1/chat")
async def chat(req: ChatRequest):
    """AIì™€ ì±„íŒ…ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    graph = app_state.get("graph")
    if not graph:
        raise HTTPException(status_code=503, detail="AI ì—”ì§„ ë¡œë“œ ì „ì…ë‹ˆë‹¤.")

    config = {"configurable": {"thread_id": f"{req.user_id}_{req.session_id}"}}
    initial_state = {
        "messages": [HumanMessage(content=req.message)],
        "user_id": req.user_id,
        "session_id": req.session_id,
        "permissions": {"allow_vision": False},
        "sensory_data": {}
    }

    try:
        final_state = await graph.ainvoke(initial_state, config=config)
        return {
            "status": "success", 
            "response": final_state["messages"][-1].content
        }
    except Exception as e:
        logger.error(f"ì±„íŒ… ì—ëŸ¬: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    # 8080 í¬íŠ¸ë¡œ ì‹¤í–‰
    uvicorn.run("supporter_ai.main:app", host="0.0.0.0", port=8080, reload=True)